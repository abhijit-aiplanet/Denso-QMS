================================================================================
DENSO QUALITY MANAGEMENT SYSTEM
Backend Architecture Overview
================================================================================

Version: 1.0
Date: October 2024

================================================================================
INTRODUCTION
================================================================================

This document explains the backend architecture approach for the Denso Quality
Management System. The system is designed to automate root cause analysis for
manufacturing defects using artificial intelligence and data integration from
multiple sources.

The core idea is simple: when a customer complaint arrives, instead of spending
weeks manually investigating the issue, our system automatically analyzes
production data, change logs, and historical patterns to identify the root cause
and recommend fixes within minutes.

================================================================================
1. OVERALL ARCHITECTURE APPROACH
================================================================================

The backend is built using a microservices architecture, which means we break
down the system into smaller, independent services that each handle a specific
responsibility. Think of it like a factory where different departments handle
different tasks - one for receiving complaints, one for tracking production
history, one for analyzing changes, and so on. These services communicate with
each other to complete the full analysis.

This approach gives us several advantages. First, if one service needs
maintenance or has an issue, the others can continue working. Second, we can
scale different parts of the system independently based on demand. Third, we can
update or improve one service without affecting the others. Finally, different
teams can work on different services simultaneously, speeding up development.

The architecture has several distinct layers, each with a specific purpose:

At the top, we have the user interface layer where quality engineers, managers,
and operators interact with the system through web browsers or mobile apps. This
is what users see and click on.

Below that is the API gateway, which acts as the single entry point for all
requests. It handles authentication (making sure users are who they say they
are), routes requests to the right service, and implements rate limiting to
prevent system overload. Think of it as the front desk receptionist who directs
visitors to the right department.

The heart of the system is the application services layer, where we have
multiple specialized services. The complaint service manages all customer
complaints and tracks their status. The traceability service connects to Denso's
production databases to find out exactly when, where, and how a defective unit
was manufactured. The analytics service performs statistical analysis to find
patterns and correlations. The reporting service generates reports and exports.
The workflow service manages the step-by-step process from complaint to
resolution. The notification service sends alerts and updates to relevant people.

Beneath the application services is the AI agent system, which is the
intelligence layer. This is where eight specialized AI agents work in parallel
to analyze different aspects of the problem. We'll explain this in detail in the
next section, but the key point is that these agents work simultaneously rather
than sequentially, which is why we can complete analysis in minutes instead of
weeks.

At the foundation, we have the data layer with multiple types of databases. We
use a relational database for structured data like complaints, user accounts,
and action plans. We use a time-series database specifically optimized for
tracking metrics over time, like defect rates and quality KPIs. We use a
document database for flexible data structures like logs and cached information.
We use an in-memory cache for lightning-fast access to frequently used data. We
use a search engine for finding similar historical cases quickly. And we use
cloud storage for large files like images, PDFs, and generated reports.

All these services communicate through a message queue system, which allows them
to work asynchronously. When one service completes its work, it doesn't directly
call the next service. Instead, it publishes a message saying "I'm done, here's
my result," and interested services pick up that message and do their part. This
loose coupling makes the system more resilient and scalable.

Finally, we integrate with external systems - Denso One Database for production
traceability, Aikyan for change management logs, MES and LIMS for real-time
manufacturing data, and SAP for material and cost information. These
integrations pull in the data our AI agents need to perform their analysis.

================================================================================
2. THE MULTI-AGENT INTELLIGENCE SYSTEM
================================================================================

The most innovative part of our architecture is the multi-agent system. Instead
of building one large program that does everything sequentially, we created
eight specialized AI agents, each an expert in one specific aspect of root cause
analysis. They work in parallel like a team of specialists simultaneously
investigating different angles of a problem.

Here's how the agent system works at a high level:

When a new complaint arrives, an orchestrator component receives it and creates
an execution plan. The orchestrator is like a project manager - it knows which
agents need to run, in what order, and which agents depend on others. It assigns
tasks to agents, monitors their progress, handles failures, and aggregates all
the results at the end.

The agents themselves are independent programs that receive a task, process it
using AI and data analysis, and return results. Some agents can run immediately
in parallel, while others need to wait for previous agents to finish because
they depend on their results.

Let's walk through each agent and what it does:

THE INTAKE AGENT is the first to run. It reads the customer complaint text
(which might be messy, incomplete, or inconsistent) and uses natural language
processing to extract structured information. It identifies part numbers, serial
numbers, dates, defect types, and even assesses the urgency based on the tone
and content. Think of it as an intelligent form filler that can read free text
and pull out the important details. If the complaint says "The CHE-MIDDLE-5042
part from last week's lot is leaking," the Intake Agent extracts the part
number, identifies "leaking" as the defect type, and figures out the approximate
date. This structured data is then used by all the other agents.

THE TRACEABILITY AGENT connects to Denso's production systems to find the
complete manufacturing history of the defective unit. Given a serial number, it
traces back to find which production line made it, which specific machine, which
operator was on shift, what date and time, which lot it belonged to, what the
test results were, and what child parts were used (like tubes, sealant, flux).
It even looks up the suppliers of those child parts and their lot numbers. This
creates a complete picture of the unit's production journey. The Traceability
Agent also finds other units from the same lot to see if they had similar
issues, which helps identify whether it's an isolated incident or a systemic
problem.

THE CHANGE SCAN AGENT looks at what changed in the factory around the time the
defect occurred. It connects to the change management system (Aikyan) and
retrieves all process changes in a time window (typically a week or two before
the defect). It categorizes these changes using the 5M1E framework: Man
(operator changes, training), Machine (equipment modifications, calibration),
Material (supplier changes, raw material specs), Method (process changes, work
instructions), Measurement (test equipment changes), and Environment (facility
conditions). For each change, it scores how relevant it is based on timing,
location, and the affected components. The output is a timeline of relevant
changes that might have caused the defect.

THE CORRELATION AGENT performs statistical analysis to determine which changes
are actually correlated with defect rate increases. Just because a change
happened around the time doesn't mean it caused the problem - correlation
doesn't equal causation, but it's a strong indicator. This agent pulls quality
metrics time series data and runs various statistical tests (correlation
coefficients, significance testing, time-lag analysis) to see which changes show
the strongest mathematical relationship with the defect spike. It calculates
confidence scores and p-values to quantify how certain we can be about each
correlation. For example, if flux weight was reduced three days before leak
rates jumped, and the correlation coefficient is very high with strong
statistical significance, that change becomes a prime suspect.

THE BENCHMARKING AGENT is the system's memory. It searches through all
historical complaints to find similar cases and see what solutions worked before.
It uses advanced semantic search technology that understands meaning, not just
keywords. So if a previous case described "insufficient brazing seal" and the
current case says "welding gap issues," it recognizes these are related even
though the words are different. For each similar historical case, it retrieves
what root cause was found, what countermeasures were implemented, how effective
they were, and whether the problem recurred. This allows the system to learn
from past experience and recommend solutions that have proven successful.

THE HYPOTHESIS AGENT synthesizes all the information from previous agents to
form intelligent guesses about the root cause. It takes the traceability data,
the list of relevant changes, the correlation analysis, and the similar
historical cases, and uses large language model AI to generate plausible root
cause hypotheses. Each hypothesis includes a confidence score based on the
strength of supporting evidence. For example, it might generate a hypothesis
like "Flux weight reduction from 5.2g to 4.8g is causing insufficient brazing,
leading to leak failures" with 85% confidence because there's a strong
correlation, a relevant change, and historical precedent. The Hypothesis Agent
ranks multiple possibilities so engineers can investigate the most likely causes
first.

THE ACTION PLANNER AGENT takes the top hypotheses and recommends specific
countermeasures. It doesn't just identify problems - it suggests concrete
actions to fix them. Based on the hypothesis about flux weight, it might
recommend: "Restore flux dispensing to 5.2g, run a trial lot of 100 units, test
for leaks, compare results to target." It also suggests who should be
responsible, what the deadline should be, and how to verify the fix worked. The
Action Planner learns from historical data about which types of solutions are
most effective for different types of problems.

THE KPI REPORTER AGENT is the verification system. After countermeasures are
implemented, it continuously monitors quality metrics to see if the problem is
actually fixed. It tracks defect rates, first pass yield, mean time to repair,
and other KPIs before and after the fix. It compares the results statistically
to determine if the improvement is real or just random variation. If the metrics
show sustained improvement, it confirms the solution worked. If not, it alerts
the team that additional action is needed.

The key to this multi-agent approach is parallelization and specialization. The
Traceability Agent and Change Scan Agent run at the same time because they don't
depend on each other. The Benchmarking Agent also runs in parallel. Only when
these three finish does the Correlation Agent start, because it needs their
results. This parallel execution is why we can complete in under two minutes
what would take humans days or weeks.

The orchestrator coordinates all of this. It tracks which agents have completed,
which are running, which are waiting, and which failed. If an agent fails, the
orchestrator can retry it or proceed with partial results. When all agents
finish, the orchestrator combines their outputs into a unified analysis that's
presented to the user.

================================================================================
3. DATA INTEGRATION STRATEGY
================================================================================

The AI agents need data from multiple sources to perform their analysis. Our
integration strategy focuses on pulling data from existing Denso systems without
disrupting them.

For production traceability, we integrate with the Denso One Database through
its API. When the Traceability Agent needs information about a serial number, it
makes a secure API call requesting the production record, bill of materials, and
test results. The One Database responds with this data, which we cache for an
hour to improve performance and reduce load on their system. If the API is
temporarily unavailable, we fall back to cached data from the last successful
query.

For change management, we integrate with Aikyan in two ways. The primary method
is real-time API queries where we request changes for a specific date range and
production line. The backup method is scheduled batch synchronization every
fifteen minutes, where we pull all recent changes and store them in our
database. This ensures we always have access to change data even if the Aikyan
API is down.

For real-time manufacturing data, we integrate with MES (Manufacturing Execution
System) and LIMS (Laboratory Information Management System) using webhook events.
These systems send us notifications whenever significant events occur - a unit
completes production, a test is performed, a defect is detected. We receive
these events in real-time, store them in our time-series database, and use them
for trend analysis and KPI monitoring. We also periodically poll these systems
to ensure we haven't missed any events.

For material and cost information, we integrate with SAP through a daily batch
synchronization. Every night, we pull the latest material master data, supplier
information, and cost data. This data doesn't change frequently, so daily sync
is sufficient.

All external integrations follow a consistent pattern: authenticate securely,
make the request, validate the response, transform the data to our internal
format, store it in the appropriate database, and handle errors gracefully. We
implement retry logic with exponential backoff for transient failures, circuit
breakers to prevent cascading failures, and fallback mechanisms to continue
operating with cached or partial data.

The data we collect flows through the system in a specific path. When a
complaint is submitted, it's stored in our relational database with a unique ID.
An event is published to our message queue saying "new complaint created." The
agent orchestrator picks up this event and begins the analysis workflow. As each
agent completes its work, it stores its results in the database and publishes an
event. Other agents subscribe to these events and start their work when
dependencies are met. When all agents finish, the orchestrator aggregates the
results, stores the final analysis, and notifies the user.

For time-series data like quality metrics, we use a specialized database
optimized for temporal queries. When the KPI Reporter Agent needs to compare
defect rates before and after a fix, it can efficiently query for "average
defect rate per day for the past 30 days" without scanning the entire dataset.

For historical case search, we use a search engine that's optimized for
similarity queries. All past complaints are indexed with vector embeddings that
capture their semantic meaning. When the Benchmarking Agent searches for similar
cases, it can find matches based on conceptual similarity rather than just
keyword matching.

For large files like photos of defects, PDF reports, and data exports, we store
them in cloud object storage. The database only stores metadata and a reference
to the file location.

================================================================================
4. TECHNOLOGY CHOICES
================================================================================

Our technology choices are driven by three principles: use proven technologies
with strong community support, prefer open standards and avoid vendor lock-in
where possible, and select the right tool for each specific job rather than
trying to use one database for everything.

For the application services layer, we use a mix of programming languages based
on each service's needs. Services that do heavy data processing and statistical
analysis are built with Python because it has excellent libraries for data
science and machine learning. Services that handle many simultaneous connections
and I/O operations are built with Node.js because it excels at concurrent
processing. Both languages are mature, well-supported, and have large talent
pools.

For the database layer, we use multiple database types because different data
has different characteristics. Structured transactional data like complaints,
users, and action plans goes in a traditional relational database that ensures
data consistency and supports complex queries. Time-series data like quality
metrics goes in a specialized time-series database that can efficiently store
and query temporal data with automatic compression and retention policies.
Flexible document data like logs and cache responses goes in a document database
that doesn't require a fixed schema. Frequently accessed data that needs
sub-millisecond response times goes in an in-memory cache. Full-text search and
similarity search goes in a search engine optimized for those operations.

For communication between services, we use a message queue system that provides
reliable, ordered delivery of messages. This allows services to communicate
asynchronously, which means they don't have to wait for each other and can
continue processing even if another service is temporarily down.

For AI and machine learning, we use a combination of specialized libraries and
models. Natural language processing uses established NLP libraries with custom
models trained on manufacturing domain data. Statistical analysis uses scientific
computing libraries with proven algorithms. Large language model capabilities
use API-based services from providers who specialize in this technology. Vector
similarity search uses optimized libraries that can quickly find similar items
among millions of records.

For infrastructure, we use cloud services that provide managed versions of these
technologies, so we don't have to manage servers, storage, and networking
manually. The cloud provider handles scaling, backups, security patches, and
high availability, allowing our team to focus on application logic rather than
infrastructure management.

For deployment, we use containerization, which packages each service with all
its dependencies into a portable container. These containers run on an
orchestration platform that automatically handles deployment, scaling, health
monitoring, and restart of failed containers. This approach makes deployment
consistent across development, testing, and production environments.

For monitoring and troubleshooting, we collect logs from all services in a
centralized logging system where we can search and analyze them. We collect
metrics about system performance, resource usage, and business operations in a
metrics system with dashboards and alerts. We implement distributed tracing so
we can follow a request's path through multiple services and identify
bottlenecks.

The key point is that all these technologies work together to create a cohesive
system. The user doesn't see Python or Node.js or databases - they just see a
fast, intelligent system that analyzes their complaints and provides
recommendations.

================================================================================
5. API DESIGN APPROACH
================================================================================

The system exposes its functionality through well-defined APIs (Application
Programming Interfaces) that allow the frontend, external systems, and future
integrations to interact with the backend.

Our API design follows REST principles, which means we use standard HTTP methods
and conventions that developers worldwide understand. Creating a new complaint
is a POST request, retrieving complaint details is a GET request, updating a
complaint is a PUT request, and so on. All responses are in JSON format, which
is easy for programs to parse and humans to read.

Every API endpoint is documented with its purpose, required parameters, example
requests, example responses, and possible error conditions. This documentation
is automatically generated from the code, so it's always up to date.

Authentication is handled through tokens. When a user logs in, they receive a
secure token that proves their identity. They send this token with every
subsequent request. The API gateway validates the token and ensures the user has
permission to perform the requested action.

For operations that take a long time (like running the full agent analysis), we
use an asynchronous pattern. The API immediately returns a task ID and status
"processing," then the work happens in the background. The client can poll for
status or receive a notification when the task completes. This prevents requests
from timing out and provides a better user experience.

Rate limiting prevents any single user or system from overwhelming the API with
too many requests. If someone exceeds their rate limit, they receive a clear
error message explaining the limit and when they can try again.

Error responses are informative and actionable. Instead of just saying "error
occurred," we provide a specific error code, a human-readable message, and
guidance on how to fix the issue. For example, if a required field is missing,
we tell you which field and what format it should be in.

Versioning is built into the API URLs (like /api/v1/complaints), so we can
introduce new features or changes without breaking existing integrations. Old
versions continue to work while new clients adopt the new version.

================================================================================
6. HOW IT ALL WORKS TOGETHER
================================================================================

Let's walk through a complete example to see how all these components work
together in practice.

A quality engineer receives a customer complaint about a leaking HVAC component.
They open the web application and enter the complaint details - part number,
serial number, defect description. When they click submit, here's what happens:

The web frontend sends an HTTPS request to our API gateway with the complaint
data. The API gateway validates the user's authentication token, checks their
permissions, and routes the request to the complaint service.

The complaint service validates the data, assigns a unique complaint number
(like CMP-2024-0147), stores the complaint in the database, and returns a
success response to the frontend. The user sees "Complaint submitted
successfully."

But behind the scenes, the complaint service also publishes an event to the
message queue: "complaint.created with ID CMP-2024-0147." This event triggers
the automated analysis workflow.

The agent orchestrator picks up this event and creates an execution plan. It
starts by assigning a task to the Intake Agent: "Extract structured data from
this complaint."

The Intake Agent receives the task, processes the complaint text using NLP,
extracts the part number CHE-MIDDLE-5042 and serial number SFA2-2024-06-15-
LOT247, identifies the defect type as "helium leak," and stores this structured
data. It publishes an "intake.completed" event.

The orchestrator now starts three agents in parallel. It tells the Traceability
Agent to look up the production history for serial number SFA2-2024-06-15-
LOT247. It tells the Change Scan Agent to find all changes on Line 3 between
June 8-22. It tells the Benchmarking Agent to search for historical helium leak
cases.

The Traceability Agent calls the Denso One Database API, retrieves the
production record showing this unit was made on Line 3, Machine M-3042, on June
15, and failed the helium leak test with 2.1 Pa-m³/s (threshold is 2.0). It also
gets the child part information showing flux lot FZ-2024-06-08. This takes about
three seconds. Results are stored and an event is published.

The Change Scan Agent queries Aikyan for recent changes and finds 23 changes in
the time window. It filters and scores them, identifying that on June 13, the
flux dispensing weight was reduced from 5.2g to 4.8g as part of a cost reduction
initiative. This change scores highest for relevance. This takes about seven
seconds. Results are stored and an event is published.

The Benchmarking Agent searches the historical database and finds a very similar
case from 2023 where reduced flux also caused leak failures. That case was
resolved by restoring the flux amount. This takes about two seconds. Results are
stored and an event is published.

The orchestrator sees that all three parallel agents have completed. It now
starts the Correlation Agent, which needs their combined results.

The Correlation Agent pulls defect rate data for June 8-22 from the time-series
database and performs statistical analysis. It finds a very strong correlation
(0.87) between the flux weight change on June 13 and the leak rate increase
starting June 14. The correlation is statistically significant with p-value
0.003, meaning there's less than 0.3% chance this is coincidence. This takes
about twelve seconds. Results are stored and an event is published.

The orchestrator now starts the Hypothesis Agent, which waits for all previous
agents.

The Hypothesis Agent aggregates all the evidence: traceability shows the failure,
changes show flux reduction, correlation shows strong statistical relationship,
benchmarking shows historical precedent. It uses a large language model to
generate hypotheses and ranks them by confidence. The top hypothesis is "Flux
weight reduction from 5.2g to 4.8g is causing insufficient brazing seal
formation, leading to helium leak failures" with 85% confidence. This takes
about fifteen seconds. Results are stored and an event is published.

The orchestrator starts the Action Planner Agent.

The Action Planner Agent takes the top hypothesis and generates specific
countermeasures: "Restore flux dispensing weight to 5.2g. Verify calibration of
flux dispensing unit. Run trial lot of 100 units. Test for helium leaks. Compare
results to target NG rate of <2%. Expected timeline: 3 business days. Assign to:
Process Engineering." This takes about eight seconds. Results are stored and an
event is published.

The orchestrator starts the KPI Reporter Agent.

The KPI Reporter Agent begins monitoring quality metrics. It captures the
current baseline (leak rate 4.7%, FPY 92%) and will continue tracking these
metrics daily to verify if the countermeasures work. This is ongoing.

Finally, the orchestrator aggregates all results into a complete analysis report
and publishes an "analysis.completed" event.

The notification service picks up this event and sends an email to the quality
engineer: "Analysis complete for complaint CMP-2024-0147. Root cause identified
with 85% confidence. Review recommended actions."

The web frontend, which has been showing a progress indicator, receives an
update via WebSocket and displays the complete analysis with traceability data,
change timeline, correlation charts, hypotheses, and recommended actions.

Total elapsed time: less than two minutes.

The quality engineer reviews the analysis, agrees with the recommendation, and
approves the action plan. The workflow service creates tasks, assigns them to
the process engineering team, and tracks their completion. When process
engineering completes the flux adjustment and runs the trial lot, they update
the system. The KPI Reporter Agent detects the improvement (leak rate drops to
1.8%, FPY improves to 95.5%) and confirms the fix was successful.

This entire process, from complaint submission to verified resolution, takes
days instead of weeks because of the automated analysis and intelligent
recommendations.

================================================================================
7. BENEFITS OF THIS ARCHITECTURE
================================================================================

This architecture provides several key benefits that address real manufacturing
quality management challenges.

Speed is the most obvious benefit. By using specialized AI agents that work in
parallel and automating data retrieval from multiple systems, we compress weeks
of manual investigation into minutes of automated analysis. Quality engineers
get actionable insights while the issue is still fresh rather than after the
trail has gone cold.

Consistency ensures every complaint is analyzed using the same rigorous process.
Human investigators might miss something, forget to check certain data sources,
or apply different standards. The AI agents follow the same methodology every
time, checking all relevant data sources and applying proven analysis techniques.

Accuracy improves through data-driven analysis rather than intuition. The
statistical correlation analysis objectively identifies which changes are
mathematically related to defect increases. The historical benchmarking prevents
repeating past mistakes. The confidence scoring helps engineers know which
hypotheses to investigate first.

Scalability allows the system to handle hundreds or thousands of complaints
without adding headcount. During a quality crisis when complaints spike, the
system doesn't get overwhelmed - it processes each one with the same speed and
thoroughness. As Denso expands to more plants or product lines, the system
scales accordingly.

Learning capability means the system gets smarter over time. Every resolved case
becomes training data for future cases. The benchmarking agent has more examples
to learn from. The correlation patterns inform the hypothesis generation. The
action effectiveness data guides future recommendations. This creates a
continuous improvement cycle.

Integration provides a unified view across systems. Quality engineers don't have
to log into five different systems, export data, combine spreadsheets, and
manually analyze. The system automatically pulls data from One Database, Aikyan,
MES, LIMS, and other sources, providing a complete picture in one place.

Traceability ensures we know exactly why the system made each recommendation.
The analysis shows the specific data points, statistical calculations,
historical cases, and reasoning behind each hypothesis and recommended action.
This transparency builds trust and allows engineers to validate the AI's work.

Flexibility allows the system to evolve with changing needs. New AI agents can
be added to analyze new aspects. New data sources can be integrated. New types
of analysis can be incorporated. The microservices architecture means we can
enhance one part without rebuilding everything.

================================================================================
8. SUMMARY
================================================================================

The Denso Quality Management System backend is built on a modern microservices
architecture with intelligent AI agents at its core. Instead of manually
investigating complaints over weeks, the system automatically retrieves
production data, scans change logs, performs statistical analysis, searches
historical cases, and generates root cause hypotheses with recommended
countermeasures - all within minutes.

The architecture separates concerns into specialized services and agents, each
focused on doing one thing well. These components communicate asynchronously
through message queues, allowing them to work in parallel and handle failures
gracefully. Multiple types of databases store different types of data in
optimized formats. Integration with existing Denso systems pulls in the
information needed for comprehensive analysis.

The result is a system that's fast, accurate, scalable, and continuously
learning - transforming quality management from a reactive, manual process into
a proactive, data-driven operation.

================================================================================
END OF DOCUMENT
================================================================================
